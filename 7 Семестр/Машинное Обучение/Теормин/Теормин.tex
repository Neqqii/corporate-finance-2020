\documentclass[a4paper, 12pt]{article}

%%% Матпакет
\usepackage{amsmath, amssymb, amscd, amsthm, amsfonts}
\usepackage{hyperref}
\usepackage{icomma}                  % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

%%% Страница
\usepackage{extsizes} % Возможность сделать 14-й шрифт
\usepackage{geometry} % Простой способ задавать поля
	\geometry{top=25mm}
	\geometry{bottom=25mm}
	\geometry{left=18mm}
	\geometry{right=14mm}
\usepackage{indentfirst}

%%%Стили
\usepackage{xcolor}
%\usepackage{sectsty}
%\allsectionsfont{\sffamily}
\usepackage{titlesec, blindtext, color} % подключаем нужные пакеты
\definecolor{gray75}{gray}{0.44} % определяем цвет
\definecolor{darkslateblue}{RGB}{74, 64, 164}
\newcommand{\hsp}{\hspace{14pt}} % длина линии в 20pt
\titleformat{\section}[hang]{\Large\bfseries}{\thesection\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Large\bfseries\textcolor{darkslateblue}}

%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в фомулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english, russian]{babel}	% локализация и переносы

\usepackage{soul} % Модификаторы начертания
\usepackage{csquotes} % Цитаты

%%% Теоремы
\theoremstyle{plain} % Это стиль по умолчанию, его можно не переопределять.
\newtheorem{theorem}{Теорема}[section]
\newtheorem{proposition}[theorem]{Утверждение}
\newtheorem{definition}{Определение}

\theoremstyle{definition} % "Утверждение"
\newtheorem{corollary}{Следствие}[theorem]
\newtheorem{problem}{Задача}[section]

\theoremstyle{remark} % "Примечание"
\newtheorem{example}{Пример}
\newtheorem{nota}{Примечание}

%%% Работа с картинками
\usepackage{graphicx}                % Для вставки рисунков
\graphicspath{{img/}}  				% папки с картинками
\setlength\fboxsep{3pt}              % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt}             % Толщина линий рамки \fbox{}
\usepackage{wrapfig}                 % Обтекание рисунков текстом
\title{Машинное обучение \\ Теоретический минимум}
\author{ЭФ МГУ}
\date{2020}

%%% Работа с таблицами
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}                        % Длинные таблицы
\usepackage{multirow}                         % Слияние строк в таблице

\usepackage{hyperref}
\usepackage[usenames, dvipsnames, svgnames, table, rgb]{color}
\hypersetup{				                % Гиперссылки
    unicode=true,                           % русские буквы в раздела PDF
    pdftitle={Заголовок},                   % Заголовок
    pdfauthor={Автор},                      % Автор
    pdfsubject={Тема},                      % Тема
    pdfcreator={Создатель},                 % Создатель
    pdfproducer={Производитель},            % Производитель
    pdfkeywords={keyword1} {key2} {key3},   % Ключевые слова
    colorlinks=true,        	            % false: ссылки в рамках; true: цветные ссылки
    linkcolor=red,                          % внутренние ссылки
    citecolor=green,                        % на библиографию
    filecolor=magenta,                      % на файлы
    urlcolor=cyan                           % на URL
}

\begin{document}
\maketitle

\section{Матричное дифференцирование}

\section{Градиентный спуск}

\begin{definition}[Градиентный спуск]
	 метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента.
\end{definition}

Для минимизации функции в направлении градиента используются методы одномерной оптимизации, например, метод золотого сечения. Также можно искать не наилучшую точку в направлении градиента, а какую-либо лучше текущей.

Наиболее простой в реализации из всех методов локальной оптимизации. Имеет довольно слабые условия сходимости, но при этом скорость сходимости достаточно мала (линейна). Шаг градиентного метода часто используется как часть других методов оптимизации, например, метод Флетчера — Ривса.

\subsection{Gradient Descent}

Основное свойство антиградиента~--- он указывает в сторону наискорейшего убывания функции в данной точке.
Соответственно, будет логично стартовать из некоторой точки, сдвинуться в сторону антиградиента,
пересчитать антиградиент и снова сдвинуться в его сторону и т.д.
Запишем это более формально.
Пусть~$w^{(0)}$~--- начальный набор параметров~(например, нулевой или сгенерированный из некоторого
случайного распределения).
Тогда градиентный спуск состоит в повторении следующих шагов до сходимости:
\begin{equation}
\label{eq:fullgrad}
    w^{(k)}
    =
    w^{(k - 1)}
    -
    \eta_k
    \nabla Q(w^{(k - 1)}).
\end{equation}
Здесь под~$Q(w)$ понимается значение функционала ошибки для набора параметров~$w$.

Через~$\eta_k$ обозначается длина шага, которая нужна для контроля скорости движения.
Можно делать её константной: $\eta_k = c$.
При этом если длина шага слишком большая, то есть риск постоянно <<перепрыгивать>> через точку минимума,
а если шаг слишком маленький, то движение к минимуму может занять слишком много итераций.
Иногда длину шага монотонно уменьшают по мере движения~--- например, по простой формуле
\[
    \eta_k
    =
    \frac{1}{k}.
\]

\subsection{Stochastic GD}

Проблема метода градиентного спуска~\eqref{eq:fullgrad} состоит в том,
что на каждом шаге необходимо вычислять градиент всей суммы~(будем его называть полным градиентом):
\[
    \nabla_w Q(w)
    =
    \sum_{i = 1}^{\ell}
        \nabla_w q_i(w).
\]
Это может быть очень трудоёмко при больших размерах выборки.
В то же время точное вычисление градиента может быть не так уж необходимо~---
как правило, мы делаем не очень большие шаги в сторону антиградиента,
и наличие в нём неточностей не должно сильно сказаться на общей траектории.
Опишем несколько способов оценивания полного градиента.

Оценить градиент суммы функций можно градиентом одного случайно взятого слагаемого:
\[
    \nabla_w Q(w)
    \approx
    \nabla_w q_{i_k}(w),
\]
где~$i_k$~--- случайно выбранный номер слагаемого из функционала.
В этом случае мы получим метод~\textbf{стохастического
градиентного спуска}~(stochastic gradient descent, SGD):
\[
    w^{(k)} = w^{(k - 1)} - \eta_k \nabla q_{i_k}(w^{(k - 1)}).
\]

Для выпуклого и гладкого функционала может быть получена
следующая оценка:
\[
    \EE \left[
        Q(w^{(k)}) - Q(w^*)
    \right]
    =
    O(1 / \sqrt{k}).
\]
Таким образом, метод стохастического градиента имеет менее
трудоемкие итерации по сравнению с полным градиентом,
но и скорость сходимости у него существенно меньше.

Отметим одно важное преимущество метода стохастического градиентного спуска.
Для выполнения одного шага в данном методе требуется вычислить градиент лишь одного слагаемого~---
а поскольку одно слагаемое соответствует ошибке на одном объекте,
то получается, что на каждом шаге необходимо держать в памяти всего один объект из выборки.
Данное наблюдение позволяет обучать линейные модели на очень больших выборках:
можно считывать объекты с диска по одному, и по каждому делать один шаг метода SGD.

\section{Настройка гиперпараметров с помощью кросс-валидации}

\begin{definition}[Cross-validation]
	метод оценки аналитической модели и её поведения на независимых данных.
\end{definition}
	При оценке модели имеющиеся в наличии данные разбиваются на k частей. Затем на k−1 частях данных производится обучение модели, а оставшаяся часть данных используется для тестирования. Процедура повторяется k раз; в итоге каждая из k частей данных используется для тестирования. В результате получается оценка эффективности выбранной модели с наиболее равномерным использованием имеющихся данных.

\vspace{1em}
	Обычно кросс-валидация используется в ситуациях, где целью является предсказание, и хотелось бы оценить, насколько предсказывающая модель способна работать на практике. Один цикл кросс-валидации включает разбиение набора данных на части, затем построение модели на одной части (называемой тренировочным набором), и валидация модели на другой части (называемой тестовым набором). Чтобы уменьшить разброс результатов, разные циклы кросс-валидации проводятся на разных разбиениях, а результаты валидации усредняются по всем циклам.
\end{document}
